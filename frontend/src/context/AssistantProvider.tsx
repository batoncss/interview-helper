"use client";
import {
  createContext,
  useContext,
  useState,
  useRef,
  useCallback,
} from "react";
import { WSClient } from "../services/WSClient";
import { AudioProcessor } from "../services/AudioProcessor";

interface AssistantContextType {
  segments: string[];
  recording: boolean;
  startRecording: () => Promise<void>;
  stopRecording: () => Promise<void>;
  clearSegments: () => void;
}

const AssistantContext = createContext<AssistantContextType | null>(null);

export const useAssistant = () => {
  const context = useContext(AssistantContext);
  if (!context)
    throw new Error("useAssistant must be used within AssistantProvider");
  return context;
};

export const AssistantProvider = ({
  children,
}: {
  children: React.ReactNode;
}) => {
  const [segments, setSegments] = useState<string[]>([]);
  const [recording, setRecording] = useState(false);

  const wsRef = useRef<WSClient | null>(null);
  const audioRef = useRef<AudioProcessor | null>(null);

  const startRecording = useCallback(async () => {
    if (recording) return;

    const ws = new WSClient("ws://127.0.0.1:8000/ws", (msg: string) => {
      console.log("ðŸ“¨ ÐŸÑ€Ð¸ÑˆÐ»Ð¾ Ñ Ð±ÑÐºÐ°:", msg);

      requestAnimationFrame(() => {
        setSegments((prev) => {
          // ðŸŸ¡ Ð•ÑÐ»Ð¸ Ð¿Ñ€Ð¸ÑˆÐ»Ð° Ð¿ÑƒÑÑ‚Ð°Ñ ÑÑ‚Ñ€Ð¾ÐºÐ° â€” Ð½Ð°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ð¹ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚
          if (!msg.trim()) {
            // Ð½Ð¾ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚ Ð½Ðµ Ð¿ÑƒÑÑ‚Ð¾Ð¹ (Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ ÑÐ¾Ð·Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð»Ð¸ÑˆÐ½Ð¸Ðµ)
            if (prev.length === 0 || prev[prev.length - 1].trim() !== "") {
              return [...prev, ""];
            }
            return prev;
          }

          // ðŸŸ¢ Ð•ÑÐ»Ð¸ Ð¿ÐµÑ€Ð²Ñ‹Ð¹ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚ â€” ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ ÐµÐ³Ð¾
          if (prev.length === 0) {
            return [msg];
          }

          // ðŸŸ¢ Ð•ÑÐ»Ð¸ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚ Ð¿ÑƒÑÑ‚Ð¾Ð¹ (Ð½Ð¾Ð²Ñ‹Ð¹) â€” Ð·Ð°Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ ÐµÐ³Ð¾
          if (prev[prev.length - 1].trim() === "") {
            const updated = [...prev];
            updated[updated.length - 1] = msg;
            return updated;
          }

          // ðŸŸ¢ Ð˜Ð½Ð°Ñ‡Ðµ Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚
          const updated = [...prev];
          updated[updated.length - 1] = msg;
          return updated;
        });
      });
    });

    ws.connect();
    wsRef.current = ws;

    const audio = new AudioProcessor();
    await audio.start((buffer) => {
      const downsampled = AudioProcessor.downsample(
        buffer,
        audio.audioCtx?.sampleRate || 48000,
        16000,
      );
      ws.send(AudioProcessor.floatTo16BitPCM(downsampled));
    });
    audioRef.current = audio;

    setRecording(true);
  }, [recording]);

  const stopRecording = useCallback(async () => {
    await audioRef.current?.stop();
    audioRef.current = null;
    wsRef.current?.close();
    wsRef.current = null;
    setRecording(false);
  }, []);

  const clearSegments = useCallback(() => {
    setSegments([]);
  }, []);

  return (
    <AssistantContext.Provider
      value={{
        segments,
        recording,
        startRecording,
        stopRecording,
        clearSegments,
      }}
    >
      {children}
    </AssistantContext.Provider>
  );
};
